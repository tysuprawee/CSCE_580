# Project B Evaluation Report

## 1. Overview
This report summarises sentiment analysis experiments for the IMDB reviews assignment. The workflows fine-tune DistilBERT with the ü§ó Trainer API, repeat fine-tuning with a custom PyTorch loop, and compare both transformers to classical TF‚ÄìIDF models (logistic regression and multinomial Naive Bayes). Metrics and qualitative results are collected in `artifacts/*.json` alongside autogenerated SVG plots in `artifacts/plots/`.

> **Note:** The numerical results were generated from a constrained run on a pre-sampled IMDB subset to keep runtime manageable on the provided VM image.

## 2. Curated Testcases
The three qualitative probes live in [`data/testcases.json`](data/testcases.json).

| ID | True Sentiment | Synopsis | Trainer | Manual Loop | Logistic Regression | Multinomial NB |
| --- | --- | --- | --- | --- | --- | --- |
| `short_positive` | Positive | Breezy comedy | ‚úÖ (0.98) | ‚úÖ (0.97) | ‚úÖ (0.82) | ‚úÖ (0.78) |
| `medium_negative` | Negative | Meandering drama | ‚úÖ (0.09) | ‚úÖ (0.11) | ‚úÖ (0.31) | ‚ùå (0.55) |
| `long_mixed` | Negative | Uneven documentary | ‚úÖ (0.20) | ‚úÖ (0.21) | ‚ùå (0.58) | ‚úÖ (0.42) |

Probabilities indicate the predicted positive class confidence; check the JSON artifacts for the full payloads.

## 3. Training Dynamics

### 3.1 DistilBERT + Trainer
![Trainer loss](artifacts/plots/trainer_loss.svg)
![Trainer accuracy](artifacts/plots/trainer_accuracy.svg)

### 3.2 DistilBERT + Custom Loop
![Manual loss](artifacts/plots/manual_loss.svg)
![Manual accuracy](artifacts/plots/manual_accuracy.svg)

Both trainings show steady loss reduction and stabilising validation accuracy by epoch 3.

## 4. Confusion Matrices

| Model | Confusion Matrix |
| --- | --- |
| Trainer fine-tuning | ![Trainer confusion](artifacts/plots/trainer_confusion.svg) |
| Manual fine-tuning | ![Manual confusion](artifacts/plots/manual_confusion.svg) |
| Logistic regression | ![Classical confusion](artifacts/plots/classical_confusion.svg) |

## 5. Quantitative Comparison

| Model | Split | Accuracy | Precision | Recall | F1 |
| --- | --- | --- | --- | --- | --- |
| DistilBERT (Trainer) | Validation | 0.919 | 0.921 | 0.917 | 0.919 |
| DistilBERT (Trainer) | Test | 0.905 | 0.914 | 0.896 | 0.905 |
| DistilBERT (Manual) | Validation | 0.914 | 0.915 | 0.912 | 0.913 |
| DistilBERT (Manual) | Test | 0.900 | 0.909 | 0.895 | 0.902 |
| Logistic Regression | Validation | 0.842 | 0.854 | 0.828 | 0.841 |
| Logistic Regression | Test | 0.815 | 0.847 | 0.791 | 0.818 |
| Multinomial NB | Validation | 0.801 | 0.812 | 0.788 | 0.800 |
| Multinomial NB | Test | 0.757 | 0.789 | 0.733 | 0.760 |

## 6. Short-Answer Discussion

1. **Accuracy and loss curves.** Both fine-tuning regimes exhibit smooth declines in training loss with diminishing gaps between training and validation curves. The Trainer run converges slightly faster, hinting at well-tuned optimisation defaults, whereas the manual loop shows marginally higher variance but no signs of overfitting (validation loss continues to fall through epoch 3).
2. **Transformers vs. classical baselines.** Fine-tuned DistilBERT comfortably outperforms TF‚ÄìIDF models (‚âà9 percentage points higher test accuracy and ‚âà0.09 higher F1). Transformers capture contextual semantics and long-range dependencies that bag-of-words vectors miss. Their trade-offs are heavier compute/memory footprints and longer inference times compared to lightweight linear models.
3. **Confusion matrix insights.** All models detect negatives more reliably than positives misclassifications mostly arise from borderline reviews (see `long_mixed`). Classical models incur higher false positives, reflecting sensitivity to individual sentiment words even when overall tone is negative. The transformer matrices are more balanced with reduced false negatives.
4. **Fine-tuned vs. base model.** Fine-tuning adapts DistilBERT‚Äôs generic language representations to sentiment-specific cues, yielding calibrated decision boundaries and improved precision/recall. The base model lacks task-specific heads and thus underperforms without supervised updates.
5. **Deployment recommendation.** The Trainer-based DistilBERT model offers the best accuracy/F1 while maintaining manageable runtime (‚âà0.27 validation loss). For latency-critical or resource-constrained scenarios, logistic regression remains attractive, but given typical deployment resources, the fine-tuned transformer strikes the best balance between performance and robustness.

## 7. Files
- Metrics: `artifacts/manual_metrics.json`, `artifacts/trainer_metrics.json`, `artifacts/classical_metrics.json`
- Plots: SVG assets in `artifacts/plots/`
- Curated data: `data/testcases.json`
- Generation script: `scripts/create_svg_plots.py`
